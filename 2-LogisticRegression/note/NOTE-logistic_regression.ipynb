{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、[逻辑回归](../code/2-LogisticRegression)\n",
    "- [全部代码](../code/2-LogisticRegression/LogisticRegression.py)\n",
    "\n",
    "### 1、代价函数\n",
    "- ![\\left\\{ \\begin{gathered}\n",
    "  J(\\theta ) = \\frac{1}{m}\\sum\\limits_{i = 1}^m {\\cos t({h_\\theta }({x^{(i)}}),{y^{(i)}})}  \\hfill \\\\\n",
    "  \\cos t({h_\\theta }(x),y) = \\left\\{ {\\begin{array}{c}    { - \\log ({h_\\theta }(x))} \\\\    { - \\log (1 - {h_\\theta }(x))}  \\end{array} \\begin{array}{c}    {y = 1} \\\\    {y = 0}  \\end{array} } \\right. \\hfill \\\\\n",
    "\\end{gathered}  \\right.](http://latex.codecogs.com/gif.latex?%5Clarge%20%5Cleft%5C%7B%20%5Cbegin%7Bgathered%7D%20J%28%5Ctheta%20%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5Ccos%20t%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%2C%7By%5E%7B%28i%29%7D%7D%29%7D%20%5Chfill%20%5C%5C%20%5Ccos%20t%28%7Bh_%5Ctheta%20%7D%28x%29%2Cy%29%20%3D%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%7B%20-%20%5Clog%20%28%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%5C%5C%20%7B%20-%20%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%5Cend%7Barray%7D%20%5Cbegin%7Barray%7D%7Bc%7D%20%7By%20%3D%201%7D%20%5C%5C%20%7By%20%3D%200%7D%20%5Cend%7Barray%7D%20%7D%20%5Cright.%20%5Chfill%20%5C%5C%20%5Cend%7Bgathered%7D%20%5Cright.)\n",
    "- 可以综合起来为：\n",
    "![J(\\theta ) =  - \\frac{1}{m}\\sum\\limits_{i = 1}^m {[{y^{(i)}}\\log ({h_\\theta }({x^{(i)}}) + (1 - } {y^{(i)}})\\log (1 - {h_\\theta }({x^{(i)}})]](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D)\n",
    "其中：\n",
    "![{h_\\theta }(x) = \\frac{1}{{1 + {e^{ - x}}}}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bh_%5Ctheta%20%7D%28x%29%20%3D%20%5Cfrac%7B1%7D%7B%7B1%20%2B%20%7Be%5E%7B%20-%20x%7D%7D%7D%7D)\n",
    "- 为什么不用线性回归的代价函数表示，因为线性回归的代价函数可能是非凸的，对于分类问题，使用梯度下降很难得到最小值，上面的代价函数是凸函数\n",
    "- ![{ - \\log ({h_\\theta }(x))}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%20-%20%5Clog%20%28%7Bh_%5Ctheta%20%7D%28x%29%29%7D)的图像如下，即`y=1`时：\n",
    "![enter description here](../images/LogisticRegression_01.png)\n",
    "\n",
    "可以看出，当![{{h_\\theta }(x)}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7Bh_%5Ctheta%20%7D%28x%29%7D)趋于`1`，`y=1`,与预测值一致，此时付出的代价`cost`趋于`0`，若![{{h_\\theta }(x)}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7Bh_%5Ctheta%20%7D%28x%29%7D)趋于`0`，`y=1`,此时的代价`cost`值非常大，我们最终的目的是最小化代价值\n",
    "- 同理![{ - \\log (1 - {h_\\theta }(x))}](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%20-%20%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28x%29%29%7D)的图像如下（`y=0`）：\n",
    "![enter description here](../images/LogisticRegression_02.png)\n",
    "\n",
    "### 2、梯度\n",
    "- 同样对代价函数求偏导：\n",
    "![\\frac{{\\partial J(\\theta )}}{{\\partial {\\theta _j}}} = \\frac{1}{m}\\sum\\limits_{i = 1}^m {[({h_\\theta }({x^{(i)}}) - {y^{(i)}})x_j^{(i)}]} ](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cfrac%7B%7B%5Cpartial%20J%28%5Ctheta%20%29%7D%7D%7B%7B%5Cpartial%20%7B%5Ctheta%20_j%7D%7D%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29x_j%5E%7B%28i%29%7D%5D%7D%20)\n",
    "可以看出与线性回归的偏导数一致\n",
    "- 推到过程\n",
    "![enter description here](../images/LogisticRegression_03.jpg)\n",
    "\n",
    "### 3、正则化\n",
    "- 目的是为了防止过拟合\n",
    "- 在代价函数中加上一项![J(\\theta ) =  - \\frac{1}{m}\\sum\\limits_{i = 1}^m {[{y^{(i)}}\\log ({h_\\theta }({x^{(i)}}) + (1 - } {y^{(i)}})\\log (1 - {h_\\theta }({x^{(i)}})] + \\frac{\\lambda }{{2m}}\\sum\\limits_{j = 1}^n {\\theta _j^2} ](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D%20%2B%20%5Cfrac%7B%5Clambda%20%7D%7B%7B2m%7D%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5En%20%7B%5Ctheta%20_j%5E2%7D%20)\n",
    "- 注意j是重1开始的，因为theta(0)为一个常数项，X中最前面一列会加上1列1，所以乘积还是theta(0),feature没有关系，没有必要正则化\n",
    "- 正则化后的代价："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 代价函数\n",
    "def costFunction(initial_theta,X,y,inital_lambda):\n",
    "    m = len(y)\n",
    "    J = 0\n",
    "\n",
    "    h = sigmoid(np.dot(X,initial_theta))    # 计算h(z)\n",
    "    theta1 = initial_theta.copy()           # 因为正则化j=1从1开始，不包含0，所以复制一份，前theta(0)值为0\n",
    "    theta1[0] = 0\n",
    "\n",
    "    temp = np.dot(np.transpose(theta1),theta1)\n",
    "    J = (-np.dot(np.transpose(y),np.log(h))-np.dot(np.transpose(1-y),np.log(1-h))+temp*inital_lambda/2)/m   # 正则化的代价方程\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 正则化后的代价的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算梯度\n",
    "def gradient(initial_theta,X,y,inital_lambda):\n",
    "    m = len(y)\n",
    "    grad = np.zeros((initial_theta.shape[0]))\n",
    "\n",
    "    h = sigmoid(np.dot(X,initial_theta))# 计算h(z)\n",
    "    theta1 = initial_theta.copy()\n",
    "    theta1[0] = 0\n",
    "\n",
    "    grad = np.dot(np.transpose(X),h-y)/m+inital_lambda/m*theta1 #正则化的梯度\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.S型函数 即$h_\\theta\\left(x\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# S型函数\n",
    "def sigmoid(z):\n",
    "    h = np.zeros((len(z),1))    # 初始化，与z的长度一置\n",
    "\n",
    "    h = 1.0/(1.0+np.exp(-z))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、映射为多项式\n",
    "- 因为数据的feture可能很少，导致偏差大，所以创造出一些feture结合\n",
    "- eg:映射为2次方的形式:![1 + {x_1} + {x_2} + x_1^2 + {x_1}{x_2} + x_2^2](http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=1%20%2B%20%7Bx_1%7D%20%2B%20%7Bx_2%7D%20%2B%20x_1%5E2%20%2B%20%7Bx_1%7D%7Bx_2%7D%20%2B%20x_2%5E2)\n",
    "- 实现代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 映射为多项式\n",
    "def mapFeature(X1,X2):\n",
    "    degree = 3;                     # 映射的最高次方\n",
    "    out = np.ones((X1.shape[0],1))  # 映射后的结果数组（取代X）\n",
    "    '''\n",
    "    这里以degree=2为例，映射为1,x1,x2,x1^2,x1,x2,x2^2\n",
    "    '''\n",
    "    for i in np.arange(1,degree+1):\n",
    "        for j in range(i+1):\n",
    "            temp = X1**(i-j)*(X2**j)    #矩阵直接乘相当于matlab中的点乘.*\n",
    "            out = np.hstack((out, temp.reshape(-1,1)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6、使用`scipy`的优化方法\n",
    "- 梯度下降使用`scipy`中`optimize`中的`fmin_bfgs`函数\n",
    "- 调用scipy中的优化算法fmin_bfgs（拟牛顿法Broyden-Fletcher-Goldfarb-Shanno\n",
    " - costFunction是自己实现的一个求代价的函数，\n",
    " - initial_theta表示初始化的值,\n",
    " - fprime指定costFunction的梯度\n",
    " - args是其余测参数，以元组的形式传入，最后会将最小化costFunction的theta返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = optimize.fmin_bfgs(costFunction, initial_theta, fprime=gradient, args=(X,y,initial_lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7、运行结果\n",
    "- data1决策边界和准确度\n",
    "![enter description here](../images/LogisticRegression_04.png)\n",
    "![enter description here](../images/LogisticRegression_05.png)\n",
    "- data2决策边界和准确度\n",
    "![enter description here](../images/LogisticRegression_06.png)\n",
    "![enter description here](../images/LogisticRegression_07.png)\n",
    "\n",
    "### 8、[使用scikit-learn库中的逻辑回归模型实现](code/2-LogisticRegression)\n",
    "- [全部代码](code/2-LogisticRegression/LogisticRegression_scikit-learn.py)\n",
    "- 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 划分为训练集和测试集\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 归一化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#逻辑回归\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "predict = model.predict(x_test)\n",
    "right = sum(predict == y_test)\n",
    "\n",
    "predict = np.hstack((predict.reshape(-1,1),y_test.reshape(-1,1)))   # 将预测值和真实值放在一块，好观察\n",
    "print predict\n",
    "print ('测试集准确率：%f%%'%(right*100.0/predict.shape[0]))          #计算在测试集上的准确度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## [逻辑回归_手写数字识别_OneVsAll](code/2-LogisticRegression)\n",
    "- [全部代码](code/2-LogisticRegression/LogisticRegression_OneVsAll.py)\n",
    "\n",
    "### 1、随机显示100个数字\n",
    "- 我没有使用scikit-learn中的数据集，像素是20*20px，彩色图如下\n",
    "![enter description here](../images/LogisticRegression_08.png)\n",
    "灰度图：\n",
    "![enter description here](../images/LogisticRegression_09.png)\n",
    "- 实现代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 显示100个数字\n",
    "def display_data(imgData):\n",
    "    sum = 0\n",
    "    '''\n",
    "    显示100个数（若是一个一个绘制将会非常慢，可以将要画的数字整理好，放到一个矩阵中，显示这个矩阵即可）\n",
    "    - 初始化一个二维数组\n",
    "    - 将每行的数据调整成图像的矩阵，放进二维数组\n",
    "    - 显示即可\n",
    "    '''\n",
    "    pad = 1\n",
    "    display_array = -np.ones((pad+10*(20+pad),pad+10*(20+pad)))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            display_array[pad+i*(20+pad):pad+i*(20+pad)+20,pad+j*(20+pad):pad+j*(20+pad)+20] = (imgData[sum,:].reshape(20,20,order=\"F\"))    # order=F指定以列优先，在matlab中是这样的，python中需要指定，默认以行\n",
    "            sum += 1\n",
    "\n",
    "    plt.imshow(display_array,cmap='gray')   #显示灰度图像\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、OneVsAll\n",
    "- 如何利用逻辑回归解决多分类的问题，OneVsAll就是把当前某一类看成一类，其他所有类别看作一类，这样有成了二分类的问题了\n",
    "- 如下图，把途中的数据分成三类，先把红色的看成一类，把其他的看作另外一类，进行逻辑回归，然后把蓝色的看成一类，其他的再看成一类，以此类推...\n",
    "![enter description here](../images/LogisticRegression_10.png)\n",
    "- 可以看出大于2类的情况下，有多少类就要进行多少次的逻辑回归分类\n",
    "\n",
    "### 3、手写数字识别\n",
    "- 共有0-9，10个数字，需要10次分类\n",
    "- 由于**数据集y**给出的是`0,1,2...9`的数字，而进行逻辑回归需要`0/1`的label标记，所以需要对y处理\n",
    "- 说一下数据集，前`500`个是`0`,`500-1000`是`1`,`...`,所以如下图，处理后的`y`，**前500行的第一列是1，其余都是0,500-1000行第二列是1，其余都是0....**\n",
    "![enter description here](../images/LogisticRegression_11.png)\n",
    "- 然后调用**梯度下降算法**求解`theta`\n",
    "- 实现代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 求每个分类的theta，最后返回所有的all_theta\n",
    "def oneVsAll(X,y,num_labels,Lambda):\n",
    "    # 初始化变量\n",
    "    m,n = X.shape\n",
    "    all_theta = np.zeros((n+1,num_labels))  # 每一列对应相应分类的theta,共10列\n",
    "    X = np.hstack((np.ones((m,1)),X))       # X前补上一列1的偏置bias\n",
    "    class_y = np.zeros((m,num_labels))      # 数据的y对应0-9，需要映射为0/1的关系\n",
    "    initial_theta = np.zeros((n+1,1))       # 初始化一个分类的theta\n",
    "\n",
    "    # 映射y\n",
    "    for i in range(num_labels):\n",
    "        class_y[:,i] = np.int32(y==i).reshape(1,-1) # 注意reshape(1,-1)才可以赋值\n",
    "\n",
    "    #np.savetxt(\"class_y.csv\", class_y[0:600,:], delimiter=',')\n",
    "\n",
    "    '''遍历每个分类，计算对应的theta值'''\n",
    "    for i in range(num_labels):\n",
    "        result = optimize.fmin_bfgs(costFunction, initial_theta, fprime=gradient, args=(X,class_y[:,i],Lambda)) # 调用梯度下降的优化方法\n",
    "        all_theta[:,i] = result.reshape(1,-1)   # 放入all_theta中\n",
    "\n",
    "    all_theta = np.transpose(all_theta)\n",
    "    return all_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、预测\n",
    "- 之前说过，预测的结果是一个**概率值**，利用学习出来的`theta`代入预测的**S型函数**中，每行的最大值就是是某个数字的最大概率，所在的**列号**就是预测的数字的真实值,因为在分类时，所有为`0`的将`y`映射在第一列，为1的映射在第二列，依次类推\n",
    "- 实现代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 预测\n",
    "def predict_oneVsAll(all_theta,X):\n",
    "    m = X.shape[0]\n",
    "    num_labels = all_theta.shape[0]\n",
    "    p = np.zeros((m,1))\n",
    "    X = np.hstack((np.ones((m,1)),X))   #在X最前面加一列1\n",
    "\n",
    "    h = sigmoid(np.dot(X,np.transpose(all_theta)))  #预测\n",
    "\n",
    "    '''\n",
    "    返回h中每一行最大值所在的列号\n",
    "    - np.max(h, axis=1)返回h中每一行的最大值（是某个数字的最大概率）\n",
    "    - 最后where找到的最大概率所在的列号（列号即是对应的数字）\n",
    "    '''\n",
    "    p = np.array(np.where(h[0,:] == np.max(h, axis=1)[0]))\n",
    "    for i in np.arange(1, m):\n",
    "        t = np.array(np.where(h[i,:] == np.max(h, axis=1)[i]))\n",
    "        p = np.vstack((p,t))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、运行结果\n",
    "- 10次分类，在训练集上的准确度：\n",
    "![enter description here](../images/LogisticRegression_12.png)\n",
    "\n",
    "### 6、[使用scikit-learn库中的逻辑回归模型实现](code/2-LogisticRegression)\n",
    "- [全部代码](code/2-LogisticRegression/LogisticRegression_OneVsAll_scikit-learn.py)\n",
    "- 1、导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import io as spio\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2、加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = loadmat_data(\"data_digits.mat\")\n",
    "X = data['X']   # 获取X数据，每一行对应一个数字20x20px\n",
    "y = data['y']   # 这里读取mat文件y的shape=(5000, 1)\n",
    "y = np.ravel(y) # 调用sklearn需要转化成一维的(5000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3、拟合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, y) # 拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4、预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = model.predict(X) #预测\n",
    "print u\"预测准确度为：%f%%\"%np.mean(np.float64(predict == y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5、输出结果（在训练集上的准确度）\n",
    "![enter description here](../images/LogisticRegression_13.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
